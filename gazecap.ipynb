{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.003726Z",
     "start_time": "2025-03-10T22:51:49.115419Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from models.gazev2 import FrozenEncoder, GazeEstimationModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision.transforms.v2  as F\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from functools import partial"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toukapy/PycharmProjects/gazecaps/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-10 23:51:50.971738: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-10 23:51:50.978737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741647110.986963   23321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741647110.989388   23321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-10 23:51:50.998670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.009052Z",
     "start_time": "2025-03-10T22:51:52.006596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def angular_error_2d_fixed_origin(gt_2d, pred_2d, origin=(112, 180)):  # Adjusted Y-coordinate\n",
    "    \"\"\"Compute angular error between two 2D gaze points projected into 3D space.\"\"\"\n",
    "\n",
    "    gt_vector = np.array(gt_2d)\n",
    "    pred_vector = np.array(pred_2d)\n",
    "\n",
    "    gt_3d = np.array([gt_vector[0], gt_vector[1], 1.0])\n",
    "    pred_3d = np.array([pred_vector[0], pred_vector[1], 1.0])\n",
    "\n",
    "    # Normalize and avoid small values causing numerical issues\n",
    "    gt_norm = np.linalg.norm(gt_3d)\n",
    "    pred_norm = np.linalg.norm(pred_3d)\n",
    "\n",
    "    if gt_norm < 1e-5 or pred_norm < 1e-5:  # Avoid near-zero vectors\n",
    "        return np.nan  # Skip invalid cases\n",
    "\n",
    "    gt_3d /= gt_norm\n",
    "    pred_3d /= pred_norm\n",
    "\n",
    "    dot_product = np.clip(np.dot(gt_3d, pred_3d), -1.0, 1.0)\n",
    "    angle_rad = np.arccos(dot_product)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    return angle_deg"
   ],
   "id": "766175adaa76c86b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.059072Z",
     "start_time": "2025-03-10T22:51:52.056173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, h5_files, sequence_length=9, transform=None):\n",
    "        self.h5_files = h5_files\n",
    "        self.fids = [h5py.File(h5_file, 'r') for h5_file in h5_files]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        self.num_data = sum(\n",
    "            max(0, fid[\"face_patch\"].shape[0] - sequence_length + 1) for fid in self.fids\n",
    "        )\n",
    "        self.file_indices = np.cumsum([0] + [\n",
    "            max(0, fid[\"face_patch\"].shape[0] - sequence_length + 1) for fid in self.fids\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = np.searchsorted(self.file_indices, idx, side='right') - 1\n",
    "        local_idx = idx - self.file_indices[file_idx]\n",
    "        fid = self.fids[file_idx]\n",
    "\n",
    "        face_patches = [\n",
    "            cv2.resize(fid['face_patch'][local_idx + i], (224, 224))\n",
    "            for i in range(self.sequence_length)\n",
    "        ]\n",
    "        face_patches = torch.stack([\n",
    "            torch.tensor(patch).permute(2, 0, 1).float() / 255.0 for patch in face_patches\n",
    "        ])\n",
    "\n",
    "        if self.transform:\n",
    "            face_patches = torch.stack([self.transform(patch) for patch in face_patches])\n",
    "\n",
    "        if 'face_gaze' in fid.keys():\n",
    "            gazes = [\n",
    "                torch.tensor(fid['face_gaze'][local_idx + i]).float()\n",
    "                for i in range(self.sequence_length)\n",
    "            ]\n",
    "        else:\n",
    "            gazes = [torch.zeros(2) for _ in range(self.sequence_length)]\n",
    "        gazes = torch.stack(gazes)\n",
    "\n",
    "        return {\"face_patches\": face_patches, \"gazes\": gazes}"
   ],
   "id": "ef919c87c3be4cdf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.107434Z",
     "start_time": "2025-03-10T22:51:52.104001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss_function(predictions, labels):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error loss between predictions and ground truth labels.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.ndarray): Array of shape (B, 2) with model predictions.\n",
    "        labels (np.ndarray): Array of shape (B, T, 2) with ground truth gaze values.\n",
    "                             We assume T >= 1 and take the last time step as the target.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean squared error loss.\n",
    "    \"\"\"\n",
    "    # Use the last time step as the target (shape: (B, 2))\n",
    "    targets = labels[:, -1, :]\n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    return mse"
   ],
   "id": "2356f76a70c7b8d6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.156057Z",
     "start_time": "2025-03-10T22:51:52.151068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def angular_error_2d_fixed_origin(gt_2d, pred_2d, origin=(112, 180)):\n",
    "    \"\"\"Compute angular error between two 2D gaze points projected into 3D space.\"\"\"\n",
    "    gt_vector = np.array(gt_2d)\n",
    "    pred_vector = np.array(pred_2d)\n",
    "\n",
    "    gt_3d = np.array([gt_vector[0], gt_vector[1], 1.0])\n",
    "    pred_3d = np.array([pred_vector[0], pred_vector[1], 1.0])\n",
    "\n",
    "    # Normalize and avoid numerical issues\n",
    "    gt_norm = np.linalg.norm(gt_3d)\n",
    "    pred_norm = np.linalg.norm(pred_3d)\n",
    "    if gt_norm < 1e-5 or pred_norm < 1e-5:\n",
    "        return np.nan  # Skip invalid cases\n",
    "\n",
    "    gt_3d /= gt_norm\n",
    "    pred_3d /= pred_norm\n",
    "\n",
    "    dot_product = np.clip(np.dot(gt_3d, pred_3d), -1.0, 1.0)\n",
    "    angle_rad = np.arccos(dot_product)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    return angle_deg"
   ],
   "id": "f92f910270ab3f53",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.217990Z",
     "start_time": "2025-03-10T22:51:52.212136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_metrics_2(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the gaze estimation task.\n",
    "\n",
    "    Expects:\n",
    "        eval_pred: a tuple (predictions, label_ids) where:\n",
    "            - predictions: numpy array of shape (B, 2)\n",
    "            - label_ids: numpy array of shape (B, T, 2) (e.g., a sequence of gaze values)\n",
    "    Returns:\n",
    "        dict: A dictionary with keys \"eval_loss\" and \"eval_mean_angular_error\".\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.array(predictions)  # shape: (B, 2)\n",
    "    labels = np.array(labels)  # shape: (B, T, 2)\n",
    "\n",
    "    # Use the last time step as the target (shape: (B, 2))\n",
    "    targets = labels[:, -1, :]\n",
    "    mse_loss = np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    # Compute angular error for each sample.\n",
    "    angular_errors = []\n",
    "    for pred, label_seq in zip(predictions, labels):\n",
    "        gt = label_seq[-1]  # Use the last frame\n",
    "        # Project 2D points into 3D by appending 1.0\n",
    "        gt_3d = np.array([gt[0], gt[1], 1.0])\n",
    "        pred_3d = np.array([pred[0], pred[1], 1.0])\n",
    "        gt_norm = np.linalg.norm(gt_3d)\n",
    "        pred_norm = np.linalg.norm(pred_3d)\n",
    "        if gt_norm < 1e-5 or pred_norm < 1e-5:\n",
    "            angular_errors.append(np.nan)\n",
    "            continue\n",
    "        gt_3d /= gt_norm\n",
    "        pred_3d /= pred_norm\n",
    "        dot_product = np.clip(np.dot(gt_3d, pred_3d), -1.0, 1.0)\n",
    "        angle_rad = np.arccos(dot_product)\n",
    "        angle_deg = np.degrees(angle_rad)\n",
    "        angular_errors.append(angle_deg)\n",
    "\n",
    "    mean_angular_error = np.nanmean(angular_errors)\n",
    "\n",
    "    # Return keys with \"eval_\" prefix to match metric_for_best_model\n",
    "    return {\n",
    "        \"eval_loss\": float(mse_loss),\n",
    "        \"eval_mean_angular_error\": float(mean_angular_error)\n",
    "    }"
   ],
   "id": "6b268f556ac7c92f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:51:52.253923Z",
     "start_time": "2025-03-10T22:51:52.251996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        # Seleccionamos la etiqueta del último frame para calcular la pérdida\n",
    "        target = inputs[\"gazes\"][:, -1, :]  # (B, 2)\n",
    "        loss = F.mse_loss(outputs, target, reduction=\"none\")\n",
    "        loss = loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "id": "a8f1e629f409d93b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:52:30.470505Z",
     "start_time": "2025-03-10T22:52:28.062422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Directorio con los archivos .h5\n",
    "train_dir = 'xgaze_224/train'\n",
    "h5_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir) if f.endswith('.h5')]\n",
    "\n",
    "# Mezclar la lista de sujetos\n",
    "random.shuffle(h5_files)\n",
    "\n",
    "# Dividir en 80% para entrenamiento y 20% para validación (cada archivo es un sujeto)\n",
    "train_size = int(0.8 * len(h5_files))\n",
    "train_files = h5_files[:train_size]\n",
    "val_files = h5_files[train_size:]\n",
    "\n",
    "print(\n",
    "    f\"Total subjects: {len(h5_files)}, Training subjects: {len(train_files)}, Validation subjects: {len(val_files)}\")\n",
    "\n",
    "train_dataset = GazeDataset(train_files, transform=transform)\n",
    "val_dataset = GazeDataset(val_files, transform=transform)\n",
    "\n",
    "train_sample_size = 1000\n",
    "val_sample_size = 200\n",
    "\n",
    "train_indices = random.sample(range(len(train_dataset)), train_sample_size)\n",
    "val_indices = random.sample(range(len(val_dataset)), val_sample_size)\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "encoder = FrozenEncoder()\n",
    "model = GazeEstimationModel(encoder, output_dim=2).cuda()\n",
    "# model.load_state_dict(torch.load('best_gaze_model.pth'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "train_loader, model, optimizer, scheduler = accelerator.prepare(\n",
    "    train_loader, model, optimizer, scheduler\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_limit = 15\n",
    "patience_counter = 0\n",
    "best_model_path = 'best_gaze_model.pth'\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"face_patches\"] = torch.stack([sample[\"face_patches\"] for sample in batch])\n",
    "    data[\"gazes\"] = torch.stack([sample[\"gazes\"] for sample in batch])\n",
    "    return data\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gazecaps_convnext\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=False,\n",
    "    logging_dir=\"./logs2\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=1e-5,\n",
    "    metric_for_best_model=\"eval_loss\",  # Note the \"eval_\" prefix\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    max_grad_norm=0.01,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    push_to_hub=True,\n",
    "    hub_token=\"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\"\n",
    ")\n",
    "\n",
    "# 2) Create the Trainer (the frozen parameters won't update during training)\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics_2\n",
    ")\n",
    "\n",
    "# 3) Train, updating only the head\n",
    "trainer.train()\n",
    "trainer.push_to_hub()"
   ],
   "id": "4de23ba3f4bb7dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 80, Training subjects: 64, Validation subjects: 16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 102\u001B[0m\n\u001B[1;32m     92\u001B[0m trainer \u001B[38;5;241m=\u001B[39m CustomTrainer(\n\u001B[1;32m     93\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     94\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     98\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics_2\n\u001B[1;32m     99\u001B[0m )\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# 3) Train, updating only the head\u001B[39;00m\n\u001B[0;32m--> 102\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m trainer\u001B[38;5;241m.\u001B[39mpush_to_hub()\n",
      "File \u001B[0;32m~/PycharmProjects/gazecaps/my_env/lib/python3.12/site-packages/transformers/trainer.py:2232\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2230\u001B[0m     \u001B[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001B[39;00m\n\u001B[1;32m   2231\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39mdisable_progress_bars()\n\u001B[0;32m-> 2232\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2233\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2234\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2237\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2238\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2239\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n",
      "File \u001B[0;32m~/PycharmProjects/gazecaps/my_env/lib/python3.12/site-packages/transformers/trainer.py:2548\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2541\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2542\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2544\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2545\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2546\u001B[0m )\n\u001B[1;32m   2547\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2548\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2551\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2552\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2553\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2554\u001B[0m ):\n\u001B[1;32m   2555\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2556\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/PycharmProjects/gazecaps/my_env/lib/python3.12/site-packages/transformers/trainer.py:3698\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3695\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3697\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3698\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3700\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3701\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3702\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3703\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3704\u001B[0m ):\n",
      "Cell \u001B[0;32mIn[7], line 8\u001B[0m, in \u001B[0;36mCustomTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, **kwargs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Seleccionamos la etiqueta del último frame para calcular la pérdida\u001B[39;00m\n\u001B[1;32m      7\u001B[0m target \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgazes\u001B[39m\u001B[38;5;124m\"\u001B[39m][:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]  \u001B[38;5;66;03m# (B, 2)\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnone\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (loss, outputs) \u001B[38;5;28;01mif\u001B[39;00m return_outputs \u001B[38;5;28;01melse\u001B[39;00m loss\n",
      "File \u001B[0;32m~/PycharmProjects/gazecaps/my_env/lib/python3.12/site-packages/torch/nn/functional.py:3781\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3771\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[1;32m   3772\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3773\u001B[0m         mse_loss,\n\u001B[1;32m   3774\u001B[0m         (\u001B[38;5;28minput\u001B[39m, target),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3779\u001B[0m         reduction\u001B[38;5;241m=\u001B[39mreduction,\n\u001B[1;32m   3780\u001B[0m     )\n\u001B[0;32m-> 3781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()):\n\u001B[1;32m   3782\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   3783\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3784\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3785\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3786\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   3787\u001B[0m     )\n\u001B[1;32m   3788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb001e30fd3d8236"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
